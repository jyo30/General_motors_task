{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16fb9982",
   "metadata": {},
   "source": [
    "# P2198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89f57a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\"intents\":[\n",
    "\n",
    "    {\"tag\":\"Supplier\",\n",
    "    \"patterns\":[\"Damaged HO2S\",\"Damaged powertrain control module (PCM)\",\"Damaged PCM\"],\n",
    "    \"responses\":[\"Supplier\"],\n",
    "    \"context\":[\"\"]\n",
    "    },\n",
    "    {\"tag\":\"Dealer\",\n",
    "    \"patterns\":[\"Electrical:Short to VPWR in the harness or HO2S\",\"Water in the harness connector\",\"Open/shorted HO2S circuit\",\"Corrosion or incorrect harness connections\",\"Fuel System:Excessive fuel pressure\",\"Leaking/contaminated fuel injectors\",\"Leaking fuel pressure regulator\",\"Low fuel pressure or running out of fuel\",\"Low fuel pressure or running out of fuel\",\"Vapor recovery system\",\"Intake Air System:Air leaks after the mass air flow (MAF) sensor\",\"Vacuum leaks\",\"Positive crankcase ventilation (PCV) system\",\"Improperly seated engine oil dipstick\",\"Leaking gasket\",\"Leaking diaphragm or EGR vacuum regulator\",\"Stuck EGR valve\",\"Oil overfill\",\"Camshaft timing\",\"Cylinder compression\",\" Exhaust leaks before or near the HO2S(s)\"],\n",
    "    \"responses\":[\"Dealer\"],\n",
    "    \"context\":[\"\"]\n",
    "    }\n",
    "]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abc59ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'Supplier',\n",
       "   'patterns': ['Damaged HO2S',\n",
       "    'Damaged powertrain control module (PCM)',\n",
       "    'Damaged PCM'],\n",
       "   'responses': ['Supplier'],\n",
       "   'context': ['']},\n",
       "  {'tag': 'Dealer',\n",
       "   'patterns': ['Electrical:Short to VPWR in the harness or HO2S',\n",
       "    'Water in the harness connector',\n",
       "    'Open/shorted HO2S circuit',\n",
       "    'Corrosion or incorrect harness connections',\n",
       "    'Fuel System:Excessive fuel pressure',\n",
       "    'Leaking/contaminated fuel injectors',\n",
       "    'Leaking fuel pressure regulator',\n",
       "    'Low fuel pressure or running out of fuel',\n",
       "    'Low fuel pressure or running out of fuel',\n",
       "    'Vapor recovery system',\n",
       "    'Intake Air System:Air leaks after the mass air flow (MAF) sensor',\n",
       "    'Vacuum leaks',\n",
       "    'Positive crankcase ventilation (PCV) system',\n",
       "    'Improperly seated engine oil dipstick',\n",
       "    'Leaking gasket',\n",
       "    'Leaking diaphragm or EGR vacuum regulator',\n",
       "    'Stuck EGR valve',\n",
       "    'Oil overfill',\n",
       "    'Camshaft timing',\n",
       "    'Cylinder compression',\n",
       "    ' Exhaust leaks before or near the HO2S(s)'],\n",
       "   'responses': ['Dealer'],\n",
       "   'context': ['']}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "807c12eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_file = open('P2198.json').read()\n",
    "intents = json.loads(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d49d5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'Supplier',\n",
       "   'patterns': ['Damaged HO2S',\n",
       "    'Damaged powertrain control module (PCM)',\n",
       "    'Damaged PCM'],\n",
       "   'responses': ['Supplier'],\n",
       "   'context': ['']},\n",
       "  {'tag': 'Dealer',\n",
       "   'patterns': ['Electrical:Short to VPWR in the harness or HO2S',\n",
       "    'Water in the harness connector',\n",
       "    'Open/shorted HO2S circuit',\n",
       "    'Corrosion or incorrect harness connections',\n",
       "    'Fuel System:Excessive fuel pressure',\n",
       "    'Leaking/contaminated fuel injectors',\n",
       "    'Leaking fuel pressure regulator',\n",
       "    'Low fuel pressure or running out of fuel',\n",
       "    'Low fuel pressure or running out of fuel',\n",
       "    'Vapor recovery system',\n",
       "    'Intake Air System:Air leaks after the mass air flow (MAF) sensor',\n",
       "    'Vacuum leaks',\n",
       "    'Positive crankcase ventilation (PCV) system',\n",
       "    'Improperly seated engine oil dipstick',\n",
       "    'Leaking gasket',\n",
       "    'Leaking diaphragm or EGR vacuum regulator',\n",
       "    'Stuck EGR valve',\n",
       "    'Oil overfill',\n",
       "    'Camshaft timing',\n",
       "    'Cylinder compression',\n",
       "    ' Exhaust leaks before or near the HO2S(s)'],\n",
       "   'responses': ['Dealer'],\n",
       "   'context': ['']}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a06c5f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n",
      "[nltk_data] Error loading all: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#importing nltk and necessary downloads\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8bdb119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\shriya.r\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shriya.r\\anaconda3\\lib\\site-packages (from torch) (4.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70926438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b217e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using porterstemmer here\n",
    "stemmer = PorterStemmer()\n",
    "def stem(word):\n",
    "    return stemmer.stem(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df1f14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BagOfWords(tokenized_sentence, words):\n",
    "    sentence_words = [stem(word) for word in tokenized_sentence]\n",
    "    Bag = np.zeros(len(words), dtype=np.float32)\n",
    "    for idx, w in enumerate(words):\n",
    "        if w in sentence_words: \n",
    "            Bag[idx] = 1\n",
    "    return Bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b8f1966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 patterns\n",
      "2 tags: ['Dealer', 'Supplier']\n",
      "69 unique stemmed words: ['(', ')', ':', 'after', 'air', 'befor', 'camshaft', 'circuit', 'compress', 'connect', 'connector', 'control', 'corros', 'crankcas', 'cylind', 'damag', 'diaphragm', 'dipstick', 'egr', 'electr', 'engin', 'excess', 'exhaust', 'flow', 'fuel', 'gasket', 'har', 'ho2', 'improperli', 'in', 'incorrect', 'injector', 'intak', 'leak', 'leaking/contamin', 'low', 'maf', 'mass', 'modul', 'near', 'of', 'oil', 'open/short', 'or', 'out', 'overfil', 'pcm', 'pcv', 'posit', 'powertrain', 'pressur', 'recoveri', 'regul', 'run', 's', 'seat', 'sensor', 'short', 'stuck', 'system', 'the', 'time', 'to', 'vacuum', 'valv', 'vapor', 'ventil', 'vpwr', 'water']\n"
     ]
    }
   ],
   "source": [
    "all_the_words = []\n",
    "tags = []\n",
    "pair = []\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    # adding this to tag list\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenizing each word in the sentence\n",
    "        w = tokenize(pattern)\n",
    "        # addding to our words list(since w is an array we need to use extend for adding the elements)\n",
    "        all_the_words.extend(w)\n",
    "        # add to pair\n",
    "        pair.append((w, tag))\n",
    "\n",
    "# stem and lower each word. So, first excluding punctuation marks\n",
    "ignore_words = ['?', '.', '!',',']\n",
    "all_the_words = [stem(w) for w in all_the_words if w not in ignore_words]\n",
    "# remove duplicates and sort\n",
    "all_the_words = sorted(set(all_the_words))\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "print(len(pair), \"patterns\")\n",
    "print(len(tags), \"tags:\", tags)\n",
    "print(len(all_the_words), \"unique stemmed words:\", all_the_words)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30b7db6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputsize= 69\n",
      "outputsize= 2\n"
     ]
    }
   ],
   "source": [
    "# create training data\n",
    "X_train = []\n",
    "y_train = []\n",
    "#using a tuple to run through the pair\n",
    "for (pattern_sentence, tag) in pair:\n",
    "    # X-->is the bag of words for each pattern_sentence\n",
    "    bag = BagOfWords(pattern_sentence, all_the_words)\n",
    "    X_train.append(bag)\n",
    "    # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot.so defining label\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "#array\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Hyper-parameters are--->\n",
    "num_epochs = 1000\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "#len(X_train[0]) means the length of 1st bag of words because they all have the same size. \n",
    "#if we want we can just print and check. But its clear here.\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "#lets print the values\n",
    "print(\"inputsize=\",input_size)\n",
    "print(\"outputsize=\",output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d37ba701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#traning dataset\n",
    "class ChatDataset(Dataset):\n",
    "#implementing init function\n",
    "    def __init__(self):\n",
    "      #storing the values\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b782333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new class for our neural network\n",
    "class NeuralNetModel(nn.Module):\n",
    "  #This will be a feed forward neural network with two hidden layer \n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetModel, self).__init__()\n",
    "        #creating the firstlinear layer. This gets the input size and then the connected hiddenlayer\n",
    "        self.linearlayer1 = nn.Linear(input_size, hidden_size)\n",
    "        #applying batch normalization\n",
    "        self.bn1= nn.BatchNorm1d(hidden_size)\n",
    "        #creating the 1st hidden layer with input size as hiddensize and output size as the hidden size\n",
    "        self.linearlayer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2= nn.BatchNorm1d(hidden_size) \n",
    "        #creating the 2nd hidden layer with input size as hiddensize and output size as the num classes\n",
    "        self.linearlayer3 = nn.Linear(hidden_size, num_classes)\n",
    "        #using relu activation function\n",
    "        self.relu = nn.ReLU()\n",
    "    #implementing the forward pass\n",
    "    def forward(self, x):\n",
    "      #apply our first linear layer which gets x as input and then gives out the output\n",
    "        output = self.linearlayer1(x)\n",
    "        #activation function\n",
    "        output = self.relu(output)\n",
    "      #apply our first linear layer which gets output as input and then gives out the next output\n",
    "        output = self.linearlayer2(output)\n",
    "        #activation function\n",
    "        output = self.relu(output)\n",
    "        output = self.linearlayer3(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74d6c02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppose if Gpu is available, we can puish our model to the device. otherwise we can have in cpu itself\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NeuralNetModel(input_size, hidden_size, output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8041856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "682"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting the no of parameters it have. \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(model)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf7c8279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 0.8054 And Got 30 / 240 with accuracy 12.50\n",
      "Epoch [20/1000], Loss: 0.7225 And Got 64 / 480 with accuracy 13.33\n",
      "Epoch [30/1000], Loss: 0.5760 And Got 232 / 720 with accuracy 32.22\n",
      "Epoch [40/1000], Loss: 0.4641 And Got 464 / 960 with accuracy 48.33\n",
      "Epoch [50/1000], Loss: 0.3320 And Got 694 / 1200 with accuracy 57.83\n",
      "Epoch [60/1000], Loss: 0.1020 And Got 924 / 1440 with accuracy 64.17\n",
      "Epoch [70/1000], Loss: 0.2335 And Got 1154 / 1680 with accuracy 68.69\n",
      "Epoch [80/1000], Loss: 0.0644 And Got 1391 / 1920 with accuracy 72.45\n",
      "Epoch [90/1000], Loss: 0.1328 And Got 1631 / 2160 with accuracy 75.51\n",
      "Epoch [100/1000], Loss: 0.0445 And Got 1871 / 2400 with accuracy 77.96\n",
      "Epoch [110/1000], Loss: 0.0549 And Got 2111 / 2640 with accuracy 79.96\n",
      "Epoch [120/1000], Loss: 0.0252 And Got 2351 / 2880 with accuracy 81.63\n",
      "Epoch [130/1000], Loss: 0.0174 And Got 2591 / 3120 with accuracy 83.04\n",
      "Epoch [140/1000], Loss: 0.0636 And Got 2831 / 3360 with accuracy 84.26\n",
      "Epoch [150/1000], Loss: 0.0091 And Got 3071 / 3600 with accuracy 85.31\n",
      "Epoch [160/1000], Loss: 0.0234 And Got 3311 / 3840 with accuracy 86.22\n",
      "Epoch [170/1000], Loss: 0.0196 And Got 3551 / 4080 with accuracy 87.03\n",
      "Epoch [180/1000], Loss: 0.0039 And Got 3791 / 4320 with accuracy 87.75\n",
      "Epoch [190/1000], Loss: 0.0140 And Got 4031 / 4560 with accuracy 88.40\n",
      "Epoch [200/1000], Loss: 0.0130 And Got 4271 / 4800 with accuracy 88.98\n",
      "Epoch [210/1000], Loss: 0.0020 And Got 4511 / 5040 with accuracy 89.50\n",
      "Epoch [220/1000], Loss: 0.0078 And Got 4751 / 5280 with accuracy 89.98\n",
      "Epoch [230/1000], Loss: 0.0023 And Got 4991 / 5520 with accuracy 90.42\n",
      "Epoch [240/1000], Loss: 0.0022 And Got 5231 / 5760 with accuracy 90.82\n",
      "Epoch [250/1000], Loss: 0.0013 And Got 5471 / 6000 with accuracy 91.18\n",
      "Epoch [260/1000], Loss: 0.0047 And Got 5711 / 6240 with accuracy 91.52\n",
      "Epoch [270/1000], Loss: 0.0007 And Got 5951 / 6480 with accuracy 91.84\n",
      "Epoch [280/1000], Loss: 0.0029 And Got 6191 / 6720 with accuracy 92.13\n",
      "Epoch [290/1000], Loss: 0.0028 And Got 6431 / 6960 with accuracy 92.40\n",
      "Epoch [300/1000], Loss: 0.0009 And Got 6671 / 7200 with accuracy 92.65\n",
      "Epoch [310/1000], Loss: 0.0055 And Got 6911 / 7440 with accuracy 92.89\n",
      "Epoch [320/1000], Loss: 0.0008 And Got 7151 / 7680 with accuracy 93.11\n",
      "Epoch [330/1000], Loss: 0.0029 And Got 7391 / 7920 with accuracy 93.32\n",
      "Epoch [340/1000], Loss: 0.0045 And Got 7631 / 8160 with accuracy 93.52\n",
      "Epoch [350/1000], Loss: 0.0010 And Got 7871 / 8400 with accuracy 93.70\n",
      "Epoch [360/1000], Loss: 0.0021 And Got 8111 / 8640 with accuracy 93.88\n",
      "Epoch [370/1000], Loss: 0.0005 And Got 8351 / 8880 with accuracy 94.04\n",
      "Epoch [380/1000], Loss: 0.0031 And Got 8591 / 9120 with accuracy 94.20\n",
      "Epoch [390/1000], Loss: 0.0012 And Got 8831 / 9360 with accuracy 94.35\n",
      "Epoch [400/1000], Loss: 0.0003 And Got 9071 / 9600 with accuracy 94.49\n",
      "Epoch [410/1000], Loss: 0.0025 And Got 9311 / 9840 with accuracy 94.62\n",
      "Epoch [420/1000], Loss: 0.0009 And Got 9551 / 10080 with accuracy 94.75\n",
      "Epoch [430/1000], Loss: 0.0007 And Got 9791 / 10320 with accuracy 94.87\n",
      "Epoch [440/1000], Loss: 0.0004 And Got 10031 / 10560 with accuracy 94.99\n",
      "Epoch [450/1000], Loss: 0.0007 And Got 10271 / 10800 with accuracy 95.10\n",
      "Epoch [460/1000], Loss: 0.0007 And Got 10511 / 11040 with accuracy 95.21\n",
      "Epoch [470/1000], Loss: 0.0007 And Got 10751 / 11280 with accuracy 95.31\n",
      "Epoch [480/1000], Loss: 0.0008 And Got 10991 / 11520 with accuracy 95.41\n",
      "Epoch [490/1000], Loss: 0.0002 And Got 11231 / 11760 with accuracy 95.50\n",
      "Epoch [500/1000], Loss: 0.0014 And Got 11471 / 12000 with accuracy 95.59\n",
      "Epoch [510/1000], Loss: 0.0003 And Got 11711 / 12240 with accuracy 95.68\n",
      "Epoch [520/1000], Loss: 0.0011 And Got 11951 / 12480 with accuracy 95.76\n",
      "Epoch [530/1000], Loss: 0.0002 And Got 12191 / 12720 with accuracy 95.84\n",
      "Epoch [540/1000], Loss: 0.0005 And Got 12431 / 12960 with accuracy 95.92\n",
      "Epoch [550/1000], Loss: 0.0001 And Got 12671 / 13200 with accuracy 95.99\n",
      "Epoch [560/1000], Loss: 0.0001 And Got 12911 / 13440 with accuracy 96.06\n",
      "Epoch [570/1000], Loss: 0.0003 And Got 13151 / 13680 with accuracy 96.13\n",
      "Epoch [580/1000], Loss: 0.0003 And Got 13391 / 13920 with accuracy 96.20\n",
      "Epoch [590/1000], Loss: 0.0001 And Got 13631 / 14160 with accuracy 96.26\n",
      "Epoch [600/1000], Loss: 0.0001 And Got 13871 / 14400 with accuracy 96.33\n",
      "Epoch [610/1000], Loss: 0.0006 And Got 14111 / 14640 with accuracy 96.39\n",
      "Epoch [620/1000], Loss: 0.0003 And Got 14351 / 14880 with accuracy 96.44\n",
      "Epoch [630/1000], Loss: 0.0002 And Got 14591 / 15120 with accuracy 96.50\n",
      "Epoch [640/1000], Loss: 0.0001 And Got 14831 / 15360 with accuracy 96.56\n",
      "Epoch [650/1000], Loss: 0.0003 And Got 15071 / 15600 with accuracy 96.61\n",
      "Epoch [660/1000], Loss: 0.0001 And Got 15311 / 15840 with accuracy 96.66\n",
      "Epoch [670/1000], Loss: 0.0001 And Got 15551 / 16080 with accuracy 96.71\n",
      "Epoch [680/1000], Loss: 0.0002 And Got 15791 / 16320 with accuracy 96.76\n",
      "Epoch [690/1000], Loss: 0.0001 And Got 16031 / 16560 with accuracy 96.81\n",
      "Epoch [700/1000], Loss: 0.0001 And Got 16271 / 16800 with accuracy 96.85\n",
      "Epoch [710/1000], Loss: 0.0002 And Got 16511 / 17040 with accuracy 96.90\n",
      "Epoch [720/1000], Loss: 0.0001 And Got 16751 / 17280 with accuracy 96.94\n",
      "Epoch [730/1000], Loss: 0.0001 And Got 16991 / 17520 with accuracy 96.98\n",
      "Epoch [740/1000], Loss: 0.0001 And Got 17231 / 17760 with accuracy 97.02\n",
      "Epoch [750/1000], Loss: 0.0001 And Got 17471 / 18000 with accuracy 97.06\n",
      "Epoch [760/1000], Loss: 0.0002 And Got 17711 / 18240 with accuracy 97.10\n",
      "Epoch [770/1000], Loss: 0.0001 And Got 17951 / 18480 with accuracy 97.14\n",
      "Epoch [780/1000], Loss: 0.0000 And Got 18191 / 18720 with accuracy 97.17\n",
      "Epoch [790/1000], Loss: 0.0001 And Got 18431 / 18960 with accuracy 97.21\n",
      "Epoch [800/1000], Loss: 0.0000 And Got 18671 / 19200 with accuracy 97.24\n",
      "Epoch [810/1000], Loss: 0.0000 And Got 18911 / 19440 with accuracy 97.28\n",
      "Epoch [820/1000], Loss: 0.0000 And Got 19151 / 19680 with accuracy 97.31\n",
      "Epoch [830/1000], Loss: 0.0001 And Got 19391 / 19920 with accuracy 97.34\n",
      "Epoch [840/1000], Loss: 0.0001 And Got 19631 / 20160 with accuracy 97.38\n",
      "Epoch [850/1000], Loss: 0.0001 And Got 19871 / 20400 with accuracy 97.41\n",
      "Epoch [860/1000], Loss: 0.0001 And Got 20111 / 20640 with accuracy 97.44\n",
      "Epoch [870/1000], Loss: 0.0000 And Got 20351 / 20880 with accuracy 97.47\n",
      "Epoch [880/1000], Loss: 0.0000 And Got 20591 / 21120 with accuracy 97.50\n",
      "Epoch [890/1000], Loss: 0.0000 And Got 20831 / 21360 with accuracy 97.52\n",
      "Epoch [900/1000], Loss: 0.0000 And Got 21071 / 21600 with accuracy 97.55\n",
      "Epoch [910/1000], Loss: 0.0000 And Got 21311 / 21840 with accuracy 97.58\n",
      "Epoch [920/1000], Loss: 0.0000 And Got 21551 / 22080 with accuracy 97.60\n",
      "Epoch [930/1000], Loss: 0.0000 And Got 21791 / 22320 with accuracy 97.63\n",
      "Epoch [940/1000], Loss: 0.0000 And Got 22031 / 22560 with accuracy 97.66\n",
      "Epoch [950/1000], Loss: 0.0000 And Got 22271 / 22800 with accuracy 97.68\n",
      "Epoch [960/1000], Loss: 0.0000 And Got 22511 / 23040 with accuracy 97.70\n",
      "Epoch [970/1000], Loss: 0.0000 And Got 22751 / 23280 with accuracy 97.73\n",
      "Epoch [980/1000], Loss: 0.0000 And Got 22991 / 23520 with accuracy 97.75\n",
      "Epoch [990/1000], Loss: 0.0000 And Got 23231 / 23760 with accuracy 97.77\n",
      "Epoch [1000/1000], Loss: 0.0000 And Got 23471 / 24000 with accuracy 97.80\n",
      "final Accuracy-----> Got 23471 / 24000 with accuracy 97.80\n",
      "final loss: 0.0000\n",
      "training complete. file saved to p2198.pth\n"
     ]
    }
   ],
   "source": [
    "#the training dataset, we have defined earlier.\n",
    "dataset = ChatDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "#suppose if Gpu is available, we can puish our model to the device. otherwise we can have in cpu itself\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NeuralNetModel(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss is CrossEntropyLoss and optimizer is Adam\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "total=0\n",
    "correct=0\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(dtype=torch.long).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(words)\n",
    "        # if y would be one-hot, we must apply\n",
    "        # labels = torch.max(labels, 1)[1]\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scores = model(words)\n",
    "        _, pred = scores.max(1)\n",
    "        total += len(words)\n",
    "        correct += (pred==labels).sum()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f} And Got {correct} / {total} with accuracy {float(correct)/float(total)*100:.2f}')\n",
    "print(f'final Accuracy-----> Got {correct} / {total} with accuracy {float(correct)/float(total)*100:.2f}') \n",
    "print(f'final loss: {loss.item():.4f}')\n",
    "\n",
    "data = {\n",
    "\"model_state\": model.state_dict(),\n",
    "\"input_size\": input_size,\n",
    "\"hidden_size\": hidden_size,\n",
    "\"output_size\": output_size,\n",
    "\"all_words\": all_the_words,\n",
    "\"tags\": tags\n",
    "}\n",
    "#storing these  in a file, this will serialise it and save it to that file named DATA.pth.\n",
    "FILE = \"p2198.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'training complete. file saved to {FILE}')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17891c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetModel(\n",
       "  (linearlayer1): Linear(in_features=69, out_features=8, bias=True)\n",
       "  (bn1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linearlayer2): Linear(in_features=8, out_features=8, bias=True)\n",
       "  (bn2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linearlayer3): Linear(in_features=8, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the data we saved before\n",
    "FILE = \"p2198.pth\" \n",
    "data = torch.load(FILE)\n",
    "\n",
    "#defining values\n",
    "input_size = data[\"input_size\"] \n",
    "hidden_size = data[\"hidden_size\"] \n",
    "output_size = data[\"output_size\"] \n",
    "all_the_words = data['all_words'] \n",
    "tags = data['tags'] \n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "#lets print the model now.\n",
    "model = NeuralNetModel(input_size, hidden_size, output_size).to(device) \n",
    "model.load_state_dict(model_state) \n",
    "model.eval()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a45d1843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your cause\n",
      "P2198:\n",
      "GM Model: I do not understand can you please eloborate...\n",
      "P2198:Electrical\n",
      "GM Model: Dealer\n",
      "P2198::\n",
      "GM Model: Dealer\n",
      "P2198:HO2S\n",
      "GM Model: Supplier\n",
      "P2198:HO2S(s)\n",
      "GM Model: Supplier\n",
      "P2198:PCM\n",
      "GM Model: Supplier\n",
      "P2198:PCV\n",
      "GM Model: Dealer\n",
      "P2198:exit\n"
     ]
    }
   ],
   "source": [
    "bot_name = \"GM Model\"\n",
    "print(\"Please enter your cause\")\n",
    "user=\"P2198\"\n",
    "#name=input(\"Enter Your defect code \")\n",
    "\n",
    "while True:\n",
    "  #once the person types his name, then from the next chat onwards the name will be shown\n",
    "    sent=input(user+':')\n",
    "    if sent == \"exit\":\n",
    "        break\n",
    "    else:\n",
    "        sent = tokenize(sent)\n",
    "        X = BagOfWords(sent, all_the_words)\n",
    "        X = X.reshape(1, X.shape[0])\n",
    "        X = torch.from_numpy(X).to(device)\n",
    "\n",
    "        output = model(X)\n",
    "        _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "        tag = tags[predicted.item()]\n",
    "\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        prob = probs[0][predicted.item()]\n",
    "        if prob.item() > 0.85:\n",
    "            for intent in intents['intents']:\n",
    "                if tag == intent[\"tag\"]:\n",
    "                    print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: I do not understand can you please eloborate...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03da43f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GM Model :Dealer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b9f741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73726760",
   "metadata": {},
   "source": [
    "# P0102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ce6d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_file = open('P0102.json').read()\n",
    "intents = json.loads(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98f07d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'OEM',\n",
       "   'patterns': ['Damaged MAF sensor'],\n",
       "   'responses': ['OEM'],\n",
       "   'context': ['']},\n",
       "  {'tag': 'Dealer',\n",
       "   'patterns': ['MAF sensor disconnected',\n",
       "    'MAF circuit open to PCM',\n",
       "    'VPWR open to MAF sensor',\n",
       "    'PWR GND open to the MAF sensor',\n",
       "    'MAF RTN circuit open to PCM',\n",
       "    'MAF circuit shorted to GND',\n",
       "    'Intake air leak (near the MAF sensor)'],\n",
       "   'responses': ['Dealer'],\n",
       "   'context': ['']}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e01fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15cd62fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using porterstemmer here\n",
    "stemmer = PorterStemmer()\n",
    "def stem(word):\n",
    "    return stemmer.stem(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "484c80d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BagOfWords(tokenized_sentence, words):\n",
    "    sentence_words = [stem(word) for word in tokenized_sentence]\n",
    "    Bag = np.zeros(len(words), dtype=np.float32)\n",
    "    for idx, w in enumerate(words):\n",
    "        if w in sentence_words: \n",
    "            Bag[idx] = 1\n",
    "    return Bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9b3e400",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2341891979.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [3]\u001b[1;36m\u001b[0m\n\u001b[1;33m    df=try mode of [1235,3456,5678]\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def BagOfords(tokenized_sentence,words):\n",
    "    sentence_words=[stem(word) for word in tokenized_sentence]\n",
    "    Bag=np.zeros(len(words),dtype=np.float32)\n",
    "    for idx,w in enumerate(words):\n",
    "        if w in sentence_words:\n",
    "            Bag[idx]=1\n",
    "        return Bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0974845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 patterns\n",
      "2 tags: ['Dealer', 'OEM']\n",
      "20 unique stemmed words: ['(', ')', 'air', 'circuit', 'damag', 'disconnect', 'gnd', 'intak', 'leak', 'maf', 'near', 'open', 'pcm', 'pwr', 'rtn', 'sensor', 'short', 'the', 'to', 'vpwr']\n"
     ]
    }
   ],
   "source": [
    "all_the_words = []\n",
    "tags = []\n",
    "pair = []\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    # adding this to tag list\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenizing each word in the sentence\n",
    "        w = tokenize(pattern)\n",
    "        # addding to our words list(since w is an array we need to use extend for adding the elements)\n",
    "        all_the_words.extend(w)\n",
    "        # add to pair\n",
    "        pair.append((w, tag))\n",
    "\n",
    "# stem and lower each word. So, first excluding punctuation marks\n",
    "ignore_words = ['?', '.', '!',',']\n",
    "all_the_words = [stem(w) for w in all_the_words if w not in ignore_words]\n",
    "# remove duplicates and sort\n",
    "all_the_words = sorted(set(all_the_words))\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "print(len(pair), \"patterns\")\n",
    "print(len(tags), \"tags:\", tags)\n",
    "print(len(all_the_words), \"unique stemmed words:\", all_the_words)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f00e4ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputsize= 20\n",
      "outputsize= 2\n"
     ]
    }
   ],
   "source": [
    "# create training data\n",
    "X_train = []\n",
    "y_train = []\n",
    "#using a tuple to run through the pair\n",
    "for (pattern_sentence, tag) in pair:\n",
    "    # X-->is the bag of words for each pattern_sentence\n",
    "    bag = BagOfWords(pattern_sentence, all_the_words)\n",
    "    X_train.append(bag)\n",
    "    # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot.so defining label\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "#array\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Hyper-parameters are--->\n",
    "num_epochs = 1000\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "#len(X_train[0]) means the length of 1st bag of words because they all have the same size. \n",
    "#if we want we can just print and check. But its clear here.\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "#lets print the values\n",
    "print(\"inputsize=\",input_size)\n",
    "print(\"outputsize=\",output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b181f63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#traning dataset\n",
    "class ChatDataset(Dataset):\n",
    "#implementing init function\n",
    "    def __init__(self):\n",
    "      #storing the values\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a1bdca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new class for our neural network\n",
    "class NeuralNetModel(nn.Module):\n",
    "  #This will be a feed forward neural network with two hidden layer \n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetModel, self).__init__()\n",
    "        #creating the firstlinear layer. This gets the input size and then the connected hiddenlayer\n",
    "        self.linearlayer1 = nn.Linear(input_size, hidden_size)\n",
    "        #applying batch normalization\n",
    "        self.bn1= nn.BatchNorm1d(hidden_size)\n",
    "        #creating the 1st hidden layer with input size as hiddensize and output size as the hidden size\n",
    "        self.linearlayer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2= nn.BatchNorm1d(hidden_size) \n",
    "        #creating the 2nd hidden layer with input size as hiddensize and output size as the num classes\n",
    "        self.linearlayer3 = nn.Linear(hidden_size, num_classes)\n",
    "        #using relu activation function\n",
    "        self.relu = nn.ReLU()\n",
    "    #implementing the forward pass\n",
    "    def forward(self, x):\n",
    "      #apply our first linear layer which gets x as input and then gives out the output\n",
    "        output = self.linearlayer1(x)\n",
    "        #activation function\n",
    "        output = self.relu(output)\n",
    "      #apply our first linear layer which gets output as input and then gives out the next output\n",
    "        output = self.linearlayer2(output)\n",
    "        #activation function\n",
    "        output = self.relu(output)\n",
    "        output = self.linearlayer3(output)\n",
    "        return   output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63b82595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppose if Gpu is available, we can puish our model to the device. otherwise we can have in cpu itself\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NeuralNetModel(input_size, hidden_size, output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "913c0e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting the no of parameters it have. \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(model)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "640c8405",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 0.6007 And Got 70 / 80 with accuracy 87.50\n",
      "Epoch [20/1000], Loss: 0.5641 And Got 140 / 160 with accuracy 87.50\n",
      "Epoch [30/1000], Loss: 0.5256 And Got 210 / 240 with accuracy 87.50\n",
      "Epoch [40/1000], Loss: 0.4846 And Got 280 / 320 with accuracy 87.50\n",
      "Epoch [50/1000], Loss: 0.4400 And Got 350 / 400 with accuracy 87.50\n",
      "Epoch [60/1000], Loss: 0.3953 And Got 420 / 480 with accuracy 87.50\n",
      "Epoch [70/1000], Loss: 0.3530 And Got 490 / 560 with accuracy 87.50\n",
      "Epoch [80/1000], Loss: 0.3145 And Got 560 / 640 with accuracy 87.50\n",
      "Epoch [90/1000], Loss: 0.2800 And Got 630 / 720 with accuracy 87.50\n",
      "Epoch [100/1000], Loss: 0.2502 And Got 700 / 800 with accuracy 87.50\n",
      "Epoch [110/1000], Loss: 0.2243 And Got 770 / 880 with accuracy 87.50\n",
      "Epoch [120/1000], Loss: 0.2013 And Got 840 / 960 with accuracy 87.50\n",
      "Epoch [130/1000], Loss: 0.1809 And Got 910 / 1040 with accuracy 87.50\n",
      "Epoch [140/1000], Loss: 0.1625 And Got 980 / 1120 with accuracy 87.50\n",
      "Epoch [150/1000], Loss: 0.1496 And Got 1050 / 1200 with accuracy 87.50\n",
      "Epoch [160/1000], Loss: 0.1382 And Got 1120 / 1280 with accuracy 87.50\n",
      "Epoch [170/1000], Loss: 0.1290 And Got 1190 / 1360 with accuracy 87.50\n",
      "Epoch [180/1000], Loss: 0.1206 And Got 1260 / 1440 with accuracy 87.50\n",
      "Epoch [190/1000], Loss: 0.1129 And Got 1330 / 1520 with accuracy 87.50\n",
      "Epoch [200/1000], Loss: 0.1059 And Got 1406 / 1600 with accuracy 87.88\n",
      "Epoch [210/1000], Loss: 0.0994 And Got 1486 / 1680 with accuracy 88.45\n",
      "Epoch [220/1000], Loss: 0.0930 And Got 1566 / 1760 with accuracy 88.98\n",
      "Epoch [230/1000], Loss: 0.0865 And Got 1646 / 1840 with accuracy 89.46\n",
      "Epoch [240/1000], Loss: 0.0799 And Got 1726 / 1920 with accuracy 89.90\n",
      "Epoch [250/1000], Loss: 0.0733 And Got 1806 / 2000 with accuracy 90.30\n",
      "Epoch [260/1000], Loss: 0.0666 And Got 1886 / 2080 with accuracy 90.67\n",
      "Epoch [270/1000], Loss: 0.0600 And Got 1966 / 2160 with accuracy 91.02\n",
      "Epoch [280/1000], Loss: 0.0535 And Got 2046 / 2240 with accuracy 91.34\n",
      "Epoch [290/1000], Loss: 0.0473 And Got 2126 / 2320 with accuracy 91.64\n",
      "Epoch [300/1000], Loss: 0.0415 And Got 2206 / 2400 with accuracy 91.92\n",
      "Epoch [310/1000], Loss: 0.0365 And Got 2286 / 2480 with accuracy 92.18\n",
      "Epoch [320/1000], Loss: 0.0319 And Got 2366 / 2560 with accuracy 92.42\n",
      "Epoch [330/1000], Loss: 0.0280 And Got 2446 / 2640 with accuracy 92.65\n",
      "Epoch [340/1000], Loss: 0.0245 And Got 2526 / 2720 with accuracy 92.87\n",
      "Epoch [350/1000], Loss: 0.0216 And Got 2606 / 2800 with accuracy 93.07\n",
      "Epoch [360/1000], Loss: 0.0190 And Got 2686 / 2880 with accuracy 93.26\n",
      "Epoch [370/1000], Loss: 0.0168 And Got 2766 / 2960 with accuracy 93.45\n",
      "Epoch [380/1000], Loss: 0.0150 And Got 2846 / 3040 with accuracy 93.62\n",
      "Epoch [390/1000], Loss: 0.0134 And Got 2926 / 3120 with accuracy 93.78\n",
      "Epoch [400/1000], Loss: 0.0120 And Got 3006 / 3200 with accuracy 93.94\n",
      "Epoch [410/1000], Loss: 0.0108 And Got 3086 / 3280 with accuracy 94.09\n",
      "Epoch [420/1000], Loss: 0.0098 And Got 3166 / 3360 with accuracy 94.23\n",
      "Epoch [430/1000], Loss: 0.0089 And Got 3246 / 3440 with accuracy 94.36\n",
      "Epoch [440/1000], Loss: 0.0082 And Got 3326 / 3520 with accuracy 94.49\n",
      "Epoch [450/1000], Loss: 0.0075 And Got 3406 / 3600 with accuracy 94.61\n",
      "Epoch [460/1000], Loss: 0.0069 And Got 3486 / 3680 with accuracy 94.73\n",
      "Epoch [470/1000], Loss: 0.0064 And Got 3566 / 3760 with accuracy 94.84\n",
      "Epoch [480/1000], Loss: 0.0059 And Got 3646 / 3840 with accuracy 94.95\n",
      "Epoch [490/1000], Loss: 0.0055 And Got 3726 / 3920 with accuracy 95.05\n",
      "Epoch [500/1000], Loss: 0.0050 And Got 3806 / 4000 with accuracy 95.15\n",
      "Epoch [510/1000], Loss: 0.0045 And Got 3886 / 4080 with accuracy 95.25\n",
      "Epoch [520/1000], Loss: 0.0039 And Got 3966 / 4160 with accuracy 95.34\n",
      "Epoch [530/1000], Loss: 0.0035 And Got 4046 / 4240 with accuracy 95.42\n",
      "Epoch [540/1000], Loss: 0.0030 And Got 4126 / 4320 with accuracy 95.51\n",
      "Epoch [550/1000], Loss: 0.0027 And Got 4206 / 4400 with accuracy 95.59\n",
      "Epoch [560/1000], Loss: 0.0024 And Got 4286 / 4480 with accuracy 95.67\n",
      "Epoch [570/1000], Loss: 0.0022 And Got 4366 / 4560 with accuracy 95.75\n",
      "Epoch [580/1000], Loss: 0.0020 And Got 4446 / 4640 with accuracy 95.82\n",
      "Epoch [590/1000], Loss: 0.0018 And Got 4526 / 4720 with accuracy 95.89\n",
      "Epoch [600/1000], Loss: 0.0017 And Got 4606 / 4800 with accuracy 95.96\n",
      "Epoch [610/1000], Loss: 0.0016 And Got 4686 / 4880 with accuracy 96.02\n",
      "Epoch [620/1000], Loss: 0.0015 And Got 4766 / 4960 with accuracy 96.09\n",
      "Epoch [630/1000], Loss: 0.0014 And Got 4846 / 5040 with accuracy 96.15\n",
      "Epoch [640/1000], Loss: 0.0013 And Got 4926 / 5120 with accuracy 96.21\n",
      "Epoch [650/1000], Loss: 0.0012 And Got 5006 / 5200 with accuracy 96.27\n",
      "Epoch [660/1000], Loss: 0.0012 And Got 5086 / 5280 with accuracy 96.33\n",
      "Epoch [670/1000], Loss: 0.0011 And Got 5166 / 5360 with accuracy 96.38\n",
      "Epoch [680/1000], Loss: 0.0011 And Got 5246 / 5440 with accuracy 96.43\n",
      "Epoch [690/1000], Loss: 0.0010 And Got 5326 / 5520 with accuracy 96.49\n",
      "Epoch [700/1000], Loss: 0.0010 And Got 5406 / 5600 with accuracy 96.54\n",
      "Epoch [710/1000], Loss: 0.0009 And Got 5486 / 5680 with accuracy 96.58\n",
      "Epoch [720/1000], Loss: 0.0009 And Got 5566 / 5760 with accuracy 96.63\n",
      "Epoch [730/1000], Loss: 0.0009 And Got 5646 / 5840 with accuracy 96.68\n",
      "Epoch [740/1000], Loss: 0.0008 And Got 5726 / 5920 with accuracy 96.72\n",
      "Epoch [750/1000], Loss: 0.0008 And Got 5806 / 6000 with accuracy 96.77\n",
      "Epoch [760/1000], Loss: 0.0008 And Got 5886 / 6080 with accuracy 96.81\n",
      "Epoch [770/1000], Loss: 0.0007 And Got 5966 / 6160 with accuracy 96.85\n",
      "Epoch [780/1000], Loss: 0.0007 And Got 6046 / 6240 with accuracy 96.89\n",
      "Epoch [790/1000], Loss: 0.0007 And Got 6126 / 6320 with accuracy 96.93\n",
      "Epoch [800/1000], Loss: 0.0007 And Got 6206 / 6400 with accuracy 96.97\n",
      "Epoch [810/1000], Loss: 0.0006 And Got 6286 / 6480 with accuracy 97.01\n",
      "Epoch [820/1000], Loss: 0.0006 And Got 6366 / 6560 with accuracy 97.04\n",
      "Epoch [830/1000], Loss: 0.0006 And Got 6446 / 6640 with accuracy 97.08\n",
      "Epoch [840/1000], Loss: 0.0006 And Got 6526 / 6720 with accuracy 97.11\n",
      "Epoch [850/1000], Loss: 0.0006 And Got 6606 / 6800 with accuracy 97.15\n",
      "Epoch [860/1000], Loss: 0.0005 And Got 6686 / 6880 with accuracy 97.18\n",
      "Epoch [870/1000], Loss: 0.0005 And Got 6766 / 6960 with accuracy 97.21\n",
      "Epoch [880/1000], Loss: 0.0005 And Got 6846 / 7040 with accuracy 97.24\n",
      "Epoch [890/1000], Loss: 0.0005 And Got 6926 / 7120 with accuracy 97.28\n",
      "Epoch [900/1000], Loss: 0.0005 And Got 7006 / 7200 with accuracy 97.31\n",
      "Epoch [910/1000], Loss: 0.0005 And Got 7086 / 7280 with accuracy 97.34\n",
      "Epoch [920/1000], Loss: 0.0005 And Got 7166 / 7360 with accuracy 97.36\n",
      "Epoch [930/1000], Loss: 0.0004 And Got 7246 / 7440 with accuracy 97.39\n",
      "Epoch [940/1000], Loss: 0.0004 And Got 7326 / 7520 with accuracy 97.42\n",
      "Epoch [950/1000], Loss: 0.0004 And Got 7406 / 7600 with accuracy 97.45\n",
      "Epoch [960/1000], Loss: 0.0004 And Got 7486 / 7680 with accuracy 97.47\n",
      "Epoch [970/1000], Loss: 0.0004 And Got 7566 / 7760 with accuracy 97.50\n",
      "Epoch [980/1000], Loss: 0.0004 And Got 7646 / 7840 with accuracy 97.53\n",
      "Epoch [990/1000], Loss: 0.0004 And Got 7726 / 7920 with accuracy 97.55\n",
      "Epoch [1000/1000], Loss: 0.0004 And Got 7806 / 8000 with accuracy 97.58\n",
      "final Accuracy-----> Got 7806 / 8000 with accuracy 97.58\n",
      "final loss: 0.0004\n",
      "training complete. file saved to P0102.pth\n"
     ]
    }
   ],
   "source": [
    "#the training dataset, we have defined earlier.\n",
    "dataset = ChatDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "#suppose if Gpu is available, we can puish our model to the device. otherwise we can have in cpu itself\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NeuralNetModel(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss is CrossEntropyLoss and optimizer is Adam\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "total=0\n",
    "correct=0\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(dtype=torch.long).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(words)\n",
    "        # if y would be one-hot, we must apply\n",
    "        # labels = torch.max(labels, 1)[1]\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scores = model(words)\n",
    "        _, pred = scores.max(1)\n",
    "        total += len(words)\n",
    "        correct += (pred==labels).sum()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f} And Got {correct} / {total} with accuracy {float(correct)/float(total)*100:.2f}')\n",
    "print(f'final Accuracy-----> Got {correct} / {total} with accuracy {float(correct)/float(total)*100:.2f}') \n",
    "print(f'final loss: {loss.item():.4f}')\n",
    "\n",
    "data = {\n",
    "\"model_state\": model.state_dict(),\n",
    "\"input_size\": input_size,\n",
    "\"hidden_size\": hidden_size,\n",
    "\"output_size\": output_size,\n",
    "\"all_words\": all_the_words,\n",
    "\"tags\": tags\n",
    "}\n",
    "#storing these  in a file, this will serialise it and save it to that file named DATA.pth.\n",
    "FILE = \"P0102.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'training complete. file saved to {FILE}')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4790ab5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetModel(\n",
       "  (linearlayer1): Linear(in_features=20, out_features=8, bias=True)\n",
       "  (bn1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linearlayer2): Linear(in_features=8, out_features=8, bias=True)\n",
       "  (bn2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linearlayer3): Linear(in_features=8, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the data we saved before\n",
    "FILE = \"P0102.pth\" \n",
    "data = torch.load(FILE)\n",
    "\n",
    "#defining values\n",
    "input_size = data[\"input_size\"] \n",
    "hidden_size = data[\"hidden_size\"] \n",
    "output_size = data[\"output_size\"] \n",
    "all_the_words = data['all_words'] \n",
    "tags = data['tags'] \n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "#lets print the model now.\n",
    "model = NeuralNetModel(input_size, hidden_size, output_size).to(device) \n",
    "model.load_state_dict(model_state) \n",
    "model.eval()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f376f061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your cause\n"
     ]
    }
   ],
   "source": [
    "bot_name = \"GM Model\"\n",
    "#name=input(\"Enter Your defect code \")\n",
    "print(\"Please enter your cause\")\n",
    "user=\"P0102\"\n",
    "\n",
    "while True:\n",
    "  #once the person types his name, then from the next chat onwards the name will be shown\n",
    "    sent=input(user+':')\n",
    "    if sent == \"exit\":\n",
    "        break\n",
    "    else:\n",
    "        sent = tokenize(sent)\n",
    "        X = BagOfWords(sent, all_the_words)\n",
    "        X = X.reshape(1, X.shape[0])\n",
    "        X = torch.from_numpy(X).to(device)\n",
    "\n",
    "        output = model(X)\n",
    "        _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "        tag = tags[predicted.item()]\n",
    "\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        prob = probs[0][predicted.item()]\n",
    "        if prob.item() > 0.85:\n",
    "            for intent in intents['intents']:\n",
    "                if tag == intent[\"tag\"]:\n",
    "                    print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: I do not understand the cause can you please eloborate...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b69ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3403ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
